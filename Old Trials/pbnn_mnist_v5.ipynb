{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pbnn_mnist_v5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YM3-zGxLa63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pdb\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import time\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli,LogitRelaxedBernoulli\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "      tensor = tensor.sign()\n",
        "      tensor[tensor==0] = 1\n",
        "      return tensor\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "def sample2(mu, log_sigma2):\n",
        "    eps = torch.randn_like(mu)\n",
        "    s = mu + torch.exp(log_sigma2 / 2) * eps\n",
        "    return s\n",
        "  \n",
        "  \n",
        "def sample_gumbel(shape, eps=1e-20):\n",
        "    unif = torch.rand(*shape).cuda()\n",
        "    g = -torch.log(-torch.log(unif + eps))\n",
        "    return g\n",
        "\n",
        "def sample_gumbel_softmax(logits, temperature):\n",
        "    \"\"\"\n",
        "        Input:\n",
        "        logits: Tensor of log probs, shape = BS x k\n",
        "        temperature = scalar\n",
        "        \n",
        "        Output: Tensor of values sampled from Gumbel softmax.\n",
        "                These will tend towards a one-hot representation in the limit of temp -> 0\n",
        "                shape = BS x k\n",
        "    \"\"\"\n",
        "    g = sample_gumbel(logits.shape)\n",
        "    h = (g + logits)/temperature\n",
        "    h_max = h.max(dim=-1, keepdim=True)[0]\n",
        "    h = h - h_max\n",
        "    cache = torch.exp(h)\n",
        "    y = cache / cache.sum(dim=-1, keepdim=True)\n",
        "    return y\n",
        "  \n",
        "def sampling(mu,sig):\n",
        "  x = Normal(mu,sig)\n",
        "#   x = x.sample(torch.tensor([out_features]))\n",
        "#   print(x.cdf)\n",
        "  p = 1 - x.cdf(0)\n",
        "#   print((x.cdf(0))[0])\n",
        "#   p = Binarize(p)\n",
        "#   print(p[0])\n",
        "  a = ((p+1)/2).bernoulli()\n",
        "  a = a*2-1\n",
        "#   print(a[0])\n",
        "  a = torch.nn.functional.gumbel_softmax(p, tau=1, hard=True, eps=1e-10, dim=-1)\n",
        "#   \n",
        "#   l = LogitRelaxedBernoulli(torch.tensor([1.]).cuda(),p)\n",
        "#   l = l.sample()\n",
        "#   a = sample_gumbel_softmax(p,1.0)\n",
        "#   print(x[0]) \n",
        "  return p\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "class PBinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(PBinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "#         w = torch.empty_like(self.weight)\n",
        "#         self.weight.data = nn.init.uniform_(w,-1,1)\n",
        "#         theta.requires_grad_\n",
        "#         self.weight.data = ((theta+1)/2).bernoulli()\n",
        "#         self.weight.data = Binarize(self.weight.data-0.5)\n",
        "#         self.weight.data = Binarize(theta)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "#         print(input.data[0])\n",
        "      \n",
        "      \n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()  \n",
        "            \n",
        "#         self.weight.data=Binarize(self.weight.org)\n",
        "#         print(self.weight.data)\n",
        "#         print(self.weight.org)\n",
        "#         theta = self.weight\n",
        "        theta = torch.tanh(self.weight)\n",
        "#         print(theta)\n",
        "#         print(input[0])\n",
        "        \n",
        "\n",
        "#         print(input[0])\n",
        "        if input.size(1) != 784:\n",
        "          mu = nn.functional.linear(input,theta)\n",
        "          left = input**2 - (1- input**2)\n",
        "          right = theta**2 - (1-theta**2)\n",
        "          sigma = 1 - nn.functional.linear(left,right)\n",
        "        else:\n",
        "#           print((input**2)[0])\n",
        "#           print((1-(theta**2))[0])\n",
        "          mu = nn.functional.linear(input,theta)       \n",
        "          sigma = nn.functional.linear(input**2,1-(theta**2))\n",
        "        \n",
        "#         if input.size(1) == 784:\n",
        "#           input.data = Binarize(input.data)\n",
        "# #           print(input[0])\n",
        "#         input2 = input\n",
        "#         mu = nn.functional.linear(input2,theta)\n",
        "#         left = input2**2 - (1- input2**2)\n",
        "#         right = theta**2 - (1-theta**2)\n",
        "#         sigma = torch.ones_like(mu) - nn.functional.linear(left,right)\n",
        "        \n",
        "#         print(left[0])\n",
        "#         print(right[0])\n",
        "      \n",
        "      \n",
        "#           ss= (input**2)@(1-(theta**2).t())\n",
        "#           print(ss[0])\n",
        "#           print(sigma[0])\n",
        "\n",
        "#         m = mu.mean(0,True)\n",
        "        \n",
        "#         v = sigma.var(0,True)\n",
        "     \n",
        "#         mu = 0.5*(mu-m)/((v+(0.0001)).sqrt()+0.5)\n",
        "#         sigma = 0.5**2*sigma/(v+0.0001)\n",
        "\n",
        "        \n",
        "       \n",
        "        out1 = sampling(mu,sigma)\n",
        "\n",
        "        if self.out_features==10:\n",
        "          return mu\n",
        "        else:\n",
        "          return out1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(BinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        if input.size(1) != 784:\n",
        "            input.data=Binarize(input.data)\n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()\n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        out = nn.functional.linear(input, self.weight)\n",
        "        if not self.bias is None:\n",
        "            self.bias.org=self.bias.data.clone()\n",
        "            out += self.bias.view(1, -1).expand_as(out)\n",
        "#         print(self.weight)\n",
        "        p = np.count_nonzero((self.weight.data.cpu()+1)/2)/np.count_nonzero((self.weight.data.cpu()))\n",
        "        print(self.weight.data)\n",
        "        return out\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggHwTvf4MRqC",
        "colab_type": "code",
        "outputId": "dd576616-4f60-49fe-e097-99261d99a451",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli\n",
        "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "# if args.cuda:\n",
        "#     torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 32C3 - MP2 - 64C3 - Mp2 - 512FC - SM10c\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "#         self.conv1 = PBinarizeConv2d(1, 32, kernel_size=3)\n",
        "        \n",
        "#         self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         self.htanh1 = nn.Hardtanh()\n",
        "        \n",
        "#         self.conv2 = PBinarizeConv2d(32, 64, kernel_size=3)\n",
        "#         self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         self.htanh2 = nn.Hardtanh()\n",
        "        \n",
        "        self.fc1 = PBinarizeLinear(784, 512)\n",
        "#         self.htanh3 = nn.Hardtanh()\n",
        "        \n",
        "        self.fc2 = PBinarizeLinear(512, 10)\n",
        "\n",
        "\n",
        "    # 32C3 - MP2 - 64C3 - Mp2 - 512FC - SM10c\n",
        "  \n",
        "    def forward(self, x):\n",
        "      \n",
        " \n",
        "        x = x.view(x.size(0), -1)\n",
        "#         print(x.size())\n",
        "       \n",
        "        x = self.fc1(x)\n",
        "       \n",
        "        x = self.fc2(x)\n",
        "#         print(x)\n",
        "\n",
        "        \n",
        "        return x\n",
        "  \n",
        "\n",
        "model = Net()\n",
        "\n",
        "print(model)\n",
        "\n",
        "torch.cuda.device('cuda')\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    losses = []\n",
        "    trainloader = tqdm(train_loader)\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        " \n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "#         print(loss)\n",
        "\n",
        "#         if epoch%40==0:\n",
        "#             optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*0.1\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         \n",
        "        loss.backward()\n",
        "    \n",
        "#         for p in list(model.parameters()):\n",
        "#             if hasattr(p,'org'):\n",
        "#                 p.data.copy_(p.org)\n",
        "        optimizer.step()\n",
        "        \n",
        "#         for p in list(model.parameters()):\n",
        "#             if hasattr(p,'org'):\n",
        "#                 p.org.copy_(p.data.clamp_(-0.9,0.9))\n",
        "    \n",
        "        losses.append(loss.item())\n",
        "        trainloader.set_postfix(loss=np.mean(losses), epoch=epoch)\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    testloader = tqdm(test_loader)\n",
        "    for data, target in testloader:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "          data = Variable(data)\n",
        "        target = Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        \n",
        "        \n",
        "\n",
        "        testloader.set_postfix(loss=test_loss / len(test_loader.dataset),acc=str((100. *correct / len(test_loader.dataset)).numpy())+'%')\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (fc1): PBinarizeLinear(in_features=784, out_features=512, bias=True)\n",
            "  (fc2): PBinarizeLinear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxgRGAoMbkCf",
        "colab_type": "code",
        "outputId": "2e24e9c1-274c-4eb1-989b-242a83e4e289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "%%%time\n",
        "for epoch in range(10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:13<00:00, 35.24it/s, epoch=0, loss=0.945]\n",
            "100%|██████████| 79/79 [00:01<00:00, 44.29it/s, acc=88%, loss=0.00286]\n",
            "100%|██████████| 469/469 [00:12<00:00, 36.39it/s, epoch=1, loss=0.311]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.47it/s, acc=91%, loss=0.00216]\n",
            "100%|██████████| 469/469 [00:11<00:00, 40.58it/s, epoch=2, loss=0.247]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.32it/s, acc=93%, loss=0.00179]\n",
            "100%|██████████| 469/469 [00:11<00:00, 41.57it/s, epoch=3, loss=0.209]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.29it/s, acc=94%, loss=0.00154]\n",
            "100%|██████████| 469/469 [00:11<00:00, 40.66it/s, epoch=4, loss=0.185]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.47it/s, acc=94%, loss=0.00144]\n",
            "100%|██████████| 469/469 [00:11<00:00, 40.69it/s, epoch=5, loss=0.166]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.42it/s, acc=94%, loss=0.00135]\n",
            "100%|██████████| 469/469 [00:11<00:00, 40.57it/s, epoch=6, loss=0.148]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.05it/s, acc=95%, loss=0.00121]\n",
            "100%|██████████| 469/469 [00:12<00:00, 36.52it/s, epoch=7, loss=0.134]\n",
            "100%|██████████| 79/79 [00:01<00:00, 43.68it/s, acc=95%, loss=0.0012]\n",
            "100%|██████████| 469/469 [00:11<00:00, 40.86it/s, epoch=8, loss=0.124]\n",
            "100%|██████████| 79/79 [00:01<00:00, 45.09it/s, acc=95%, loss=0.00115]\n",
            "100%|██████████| 469/469 [00:11<00:00, 40.57it/s, epoch=9, loss=0.112]\n",
            "100%|██████████| 79/79 [00:01<00:00, 42.62it/s, acc=95%, loss=0.000996]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 4s, sys: 5 s, total: 2min 9s\n",
            "Wall time: 2min 18s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiDjIjU6Mf_y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "6a0ae9a4-30e3-4502-b1e8-e398ed17b4cd"
      },
      "source": [
        "a = torch.rand(5,4)\n",
        "a"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2111, 0.3660, 0.8533, 0.7804],\n",
              "        [0.6190, 0.2480, 0.4822, 0.3310],\n",
              "        [0.3542, 0.7472, 0.1691, 0.9444],\n",
              "        [0.2635, 0.7556, 0.0211, 0.7558],\n",
              "        [0.5158, 0.6163, 0.2596, 0.7621]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41XggYVRr8az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sphyNMNrfwQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "702701a3-6b2b-45aa-dacd-7d5f5aef6f6b"
      },
      "source": [
        "torch.ones_like(a)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GP1UM1ufwHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaDZ92vonUN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "60c20ca0-96b0-4093-ef8e-5a362d132b1a"
      },
      "source": [
        "a.bernoulli()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [1., 0., 0., 1.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YWe1Ti0MyMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "80b86d19-6aaa-41fb-8093-ddb57966b22d"
      },
      "source": [
        "a=Binarize(a)\n",
        "a"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqOWLfDtr-H2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2f7fc180-2971-4c82-9076-c430d0ec06ae"
      },
      "source": [
        "torch.nn.functional.gumbel_softmax(a, tau=1, hard=True, eps=1e-10, dim=-1)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 1.],\n",
              "        [0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0.],\n",
              "        [0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu8HiOmqjwdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "78423f00-aa52-44e9-ba18-90f2bfbf09f8"
      },
      "source": [
        "(a+1)/2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hkIV7ztMVyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "abf001e9-a205-47f4-ef49-530deffd15eb"
      },
      "source": [
        "a.tanh()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7616, 0.7616, 0.7616, 0.7616],\n",
              "        [0.7616, 0.7616, 0.7616, 0.7616],\n",
              "        [0.7616, 0.7616, 0.7616, 0.7616],\n",
              "        [0.7616, 0.7616, 0.7616, 0.7616],\n",
              "        [0.7616, 0.7616, 0.7616, 0.7616]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9idSm857otNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd7fc7e4-1e6b-4f5f-fe64-031272e58262"
      },
      "source": [
        "a.mean(0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUGDVCa1pZes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p= np.count_nonzero((a+1)/2,axis=0)/np.count_nonzero(a,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJMpKyyqIkdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5ace1b64-ef81-4696-c378-b215d94282f6"
      },
      "source": [
        "1-(p)**2"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m-AodLmu8D3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78a57934-b025-4000-d029-f43d75f6542d"
      },
      "source": [
        "np.count_nonzero((a+1)/2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwOCJG610Shm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mu = torch.randn(5)\n",
        "sig = torch.randn(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oUI0eo0XgN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d3454a76-3475-442a-fa05-886ddb256d79"
      },
      "source": [
        "x=Normal(mu,sig)\n",
        "1 - x.cdf(0)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000, 0.0000, 0.0000, 0.9960, 0.2989])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jw0mT8duoik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = -4.6\n",
        "v = 25936\n",
        "x = Normal(m,v)\n",
        "p = 1 - x.cdf(0)\n",
        "# s = sample_gumbel_softmax(p,1.0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjSvoApwIVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "3b4bece8-4c82-4174-c0f2-3758df32b339"
      },
      "source": [
        "w = torch.empty(3, 5)\n",
        "nn.init.uniform_(w,-1,1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.9226, -0.8018, -0.3588,  0.8282, -0.8950],\n",
              "        [-0.5052, -0.7240, -0.3942, -0.4255, -0.2595],\n",
              "        [-0.8417,  0.1117, -0.6369, -0.6278, -0.7887]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQjhRs_wIPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "b053bf4a-e9b9-457b-a231-6af4a91a7116"
      },
      "source": [
        "p.sample()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-01924b088bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sample'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZEDswQu1TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aa = Normal(m,v)\n",
        "# aa.sample(torch.tensor([20]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwtW9ld5_tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5baFgYuu39G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1 - x.cdf(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAMjkieNIlS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = RelaxedOneHotCategorical(torch.tensor([1.]),a)\n",
        "m.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEYyrZzMZkuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"# # Execute this code block to install dependencies when running on colab\n",
        "# try:\n",
        "#     import torch\n",
        "# except:\n",
        "#     from os.path import exists\n",
        "#     from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "#     platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "#     cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "#     accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#     !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "# try: \n",
        "#     import torchbearer\n",
        "# except:\n",
        "#     !pip install torchbearer\n",
        "    \n",
        "# from torchbearer import Trial\n",
        "# torchbearer_trial = Trial(model, optimizer, criterion, metrics=['loss', 'accuracy']).to('cuda:0')\n",
        "# torchbearer_trial.with_generators(train_loader, test_generator=test_loader)\n",
        "# torchbearer_trial.run(epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}