{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pbnn_mnist_v7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YM3-zGxLa63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pdb\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import time\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli,LogitRelaxedBernoulli\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "      tensor = tensor.sign()\n",
        "      tensor[tensor==0] = 1\n",
        "      return tensor\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "def sample2(mu, log_sigma2):\n",
        "    eps = torch.randn_like(mu)\n",
        "    s = mu + torch.exp(log_sigma2 / 2) * eps\n",
        "    return s\n",
        "  \n",
        "  \n",
        "def sample_gumbel(shape, eps=1e-20):\n",
        "    unif = torch.rand(*shape).cuda()\n",
        "    g = -torch.log(-torch.log(unif + eps))\n",
        "    return g\n",
        "\n",
        "def sample_gumbel_softmax(logits, temperature):\n",
        "    \"\"\"\n",
        "        Input:\n",
        "        logits: Tensor of log probs, shape = BS x k\n",
        "        temperature = scalar\n",
        "        \n",
        "        Output: Tensor of values sampled from Gumbel softmax.\n",
        "                These will tend towards a one-hot representation in the limit of temp -> 0\n",
        "                shape = BS x k\n",
        "    \"\"\"\n",
        "    g = sample_gumbel(logits.shape)\n",
        "    h = (g + logits)/temperature\n",
        "    h_max = h.max(dim=-1, keepdim=True)[0]\n",
        "    h = h - h_max\n",
        "    cache = torch.exp(h)\n",
        "    y = cache / cache.sum(dim=-1, keepdim=True)\n",
        "    return y\n",
        "  \n",
        "def sampling(mu,sig):\n",
        "  x = Normal(mu,sig)\n",
        "#   x = x.sample(torch.tensor([out_features]))\n",
        "#   print(x.cdf)\n",
        "  p = 1 - x.cdf(0)\n",
        "#   print((x.cdf(0))[0])\n",
        "#   p = Binarize(p)\n",
        "#   print(p[0])\n",
        "#   a = ((p+1)/2).bernoulli()\n",
        "#   a = a*2-1\n",
        "# #   print(a[0])\n",
        "#   a = torch.nn.functional.gumbel_softmax(p, tau=1, hard=True, eps=1e-10, dim=-1)\n",
        "#   \n",
        "#   l = LogitRelaxedBernoulli(torch.tensor([1.]).cuda(),p)\n",
        "#   l = l.sample()\n",
        "#   a = sample_gumbel_softmax(p,1.0)\n",
        "#   print(x[0]) \n",
        "  return p\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "class PBinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(PBinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "#         w = torch.empty_like(self.weight)\n",
        "#         self.weight.data = nn.init.uniform_(w,-1,1)\n",
        "#         theta.requires_grad_\n",
        "#         self.weight.data = ((theta+1)/2).bernoulli()\n",
        "#         self.weight.data = Binarize(self.weight.data-0.5)\n",
        "#         self.weight.data = Binarize(theta)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "#         print(input.data[0])\n",
        "      \n",
        "      \n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()  \n",
        "            \n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "#         print(self.weight.data)\n",
        "#         print(self.weight.org)\n",
        "#         theta = self.weight\n",
        "        theta = torch.tanh(self.weight)\n",
        "#         print(theta)\n",
        "#         print(input[0])\n",
        "        \n",
        "\n",
        "#         print(input[0])\n",
        "        if input.size(1) != 784:\n",
        "          mu = nn.functional.linear(input,theta)\n",
        "          left = input**2 - (1- input**2)\n",
        "          right = theta**2 - (1-theta**2)\n",
        "          sigma = 1 - nn.functional.linear(left,right)\n",
        "        else:\n",
        "#           print((input**2)[0])\n",
        "#           print((1-(theta**2))[0])\n",
        "          mu = nn.functional.linear(input,theta)       \n",
        "          sigma = nn.functional.linear(input**2,1-(theta**2))\n",
        "        \n",
        "#         \n",
        "#         print(mu.shape)\n",
        "        m = mu.mean(0,True)\n",
        "        \n",
        "        v = sigma.var(0,True)\n",
        "     \n",
        "        mu = 0.5*(mu-m)/((v+(0.0001)).sqrt()+0.5)\n",
        "        sigma = 0.5**2*sigma/(v+0.0001)\n",
        "\n",
        "        \n",
        "       \n",
        "        out1 = sampling(mu,sigma)\n",
        "\n",
        "        if self.out_features==10:\n",
        "          return mu\n",
        "        else:\n",
        "          return out1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PBinarizeConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(PBinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "      \n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()  \n",
        "            \n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        \n",
        "        theta = torch.tanh(self.weight)\n",
        "        \n",
        "\n",
        "        if input.size(1) != 3:\n",
        "          mu = nn.functional.conv2d(input, theta, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "          left = input**2 - (1- input**2)\n",
        "          right = theta**2 - (1-theta**2)\n",
        "          sigma = 1 - nn.functional.conv2d(left, right, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "        else:\n",
        "          mu = nn.functional.conv2d(input, theta, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "          sigma = nn.functional.conv2d(input**2, 1-(theta**2), None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "        \n",
        "#         print(mu.shape)\n",
        "        m = mu.mean((0,2,3),True)\n",
        "        \n",
        "        v = sigma.var((0,2,3)).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "     \n",
        "        mu = 0.5*(mu-m)/((v+(0.0001)).sqrt()+0.5)\n",
        "        sigma = 0.5**2*sigma/(v+0.0001)\n",
        "        \n",
        "        out1 = sampling(mu,sigma)\n",
        "\n",
        "\n",
        "#         if not self.bias is None:\n",
        "#             self.bias.org=self.bias.data.clone()\n",
        "#             out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
        "        \n",
        "        \n",
        "        return out1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggHwTvf4MRqC",
        "colab_type": "code",
        "outputId": "c2a52211-c643-4d72-86c3-38350e5505ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli\n",
        "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "# if args.cuda:\n",
        "#     torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 32C3 - MP2 - 64C3 - Mp2 - 512FC - SM10c\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = PBinarizeConv2d(1, 32, kernel_size=3)\n",
        "        self.mp1= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "      \n",
        "        self.conv2 = PBinarizeConv2d(32, 64, kernel_size=3)\n",
        "        self.mp2= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "     \n",
        "        self.fc1 = PBinarizeLinear(36864, 512)\n",
        "\n",
        "        \n",
        "        self.fc2 = PBinarizeLinear(512, 10)\n",
        "\n",
        "\n",
        "    # 32C3 - MP2 - 64C3 - Mp2 - 512FC - SM10c\n",
        "  \n",
        "    def forward(self, x):\n",
        "      \n",
        "#       print(x.shape)\n",
        "        \n",
        "      x = self.conv1(x)\n",
        "      x = self.conv2(x)\n",
        "\n",
        "      x = x.view(x.size(0), -1)\n",
        "#         print(x.size())\n",
        "\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "\n",
        "\n",
        "      return x\n",
        "  \n",
        "\n",
        "model = Net()\n",
        "\n",
        "print(model)\n",
        "\n",
        "torch.cuda.device('cuda')\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    losses = []\n",
        "    trainloader = tqdm(train_loader)\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        " \n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "#         print(output)\n",
        "#         output = output+1e-10\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "#         print(loss)\n",
        "\n",
        "#         if epoch%40==0:\n",
        "#             optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*0.1\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         \n",
        "        loss.backward()\n",
        "    \n",
        "        for p in list(model.parameters()):\n",
        "            if hasattr(p,'org'):\n",
        "                p.data.copy_(p.org)\n",
        "        optimizer.step()\n",
        "        \n",
        "        for p in list(model.parameters()):\n",
        "            if hasattr(p,'org'):\n",
        "                p.org.copy_(p.data.clamp_(-0.9,0.9))\n",
        "    \n",
        "        losses.append(loss.item())\n",
        "        trainloader.set_postfix(loss=np.mean(losses), epoch=epoch)\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    testloader = tqdm(test_loader)\n",
        "    for data, target in testloader:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "          data = Variable(data)\n",
        "        target = Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        \n",
        "        \n",
        "\n",
        "        testloader.set_postfix(loss=test_loss / len(test_loader.dataset),acc=str((100. *correct / len(test_loader.dataset)).numpy())+'%')\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): PBinarizeConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (mp1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): PBinarizeConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (mp2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): PBinarizeLinear(in_features=36864, out_features=512, bias=True)\n",
            "  (fc2): PBinarizeLinear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxgRGAoMbkCf",
        "colab_type": "code",
        "outputId": "2189e180-c534-43ff-c4ca-34372f785713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "%%%time\n",
        "for epoch in range(20):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:38<00:00, 12.34it/s, epoch=0, loss=0.426]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.93it/s, acc=93%, loss=0.00174]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.64it/s, epoch=1, loss=0.135]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.86it/s, acc=95%, loss=0.00128]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.42it/s, epoch=2, loss=0.133]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.05it/s, acc=96%, loss=0.000873]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.65it/s, epoch=3, loss=0.115]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.18it/s, acc=95%, loss=0.00109]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.43it/s, epoch=4, loss=0.0959]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.01it/s, acc=96%, loss=0.000876]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.69it/s, epoch=5, loss=0.0714]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.11it/s, acc=96%, loss=0.000863]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.43it/s, epoch=6, loss=0.078]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.13it/s, acc=96%, loss=0.00077]\n",
            "100%|██████████| 469/469 [00:38<00:00, 12.28it/s, epoch=7, loss=0.0745]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.10it/s, acc=97%, loss=0.00084]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.44it/s, epoch=8, loss=0.0662]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.01it/s, acc=96%, loss=0.000852]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.70it/s, epoch=9, loss=0.0547]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.96it/s, acc=96%, loss=0.00081]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.45it/s, epoch=10, loss=0.0594]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.12it/s, acc=96%, loss=0.000806]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.68it/s, epoch=11, loss=0.072]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.04it/s, acc=96%, loss=0.00102]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.43it/s, epoch=12, loss=0.055]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.83it/s, acc=97%, loss=0.000707]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.69it/s, epoch=13, loss=0.0443]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.10it/s, acc=97%, loss=0.00064]\n",
            "100%|██████████| 469/469 [00:38<00:00, 12.24it/s, epoch=14, loss=0.0389]\n",
            "100%|██████████| 79/79 [00:03<00:00, 23.99it/s, acc=97%, loss=0.000602]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.55it/s, epoch=15, loss=0.0461]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.98it/s, acc=96%, loss=0.000925]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.42it/s, epoch=16, loss=0.0436]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.94it/s, acc=97%, loss=0.000662]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.67it/s, epoch=17, loss=0.0324]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.90it/s, acc=97%, loss=0.000677]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.42it/s, epoch=18, loss=0.0361]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.01it/s, acc=97%, loss=0.000744]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.66it/s, epoch=19, loss=0.0435]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.76it/s, acc=97%, loss=0.000696]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 14s, sys: 3min 49s, total: 13min 3s\n",
            "Wall time: 13min 26s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiDjIjU6Mf_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.rand(3,4)\n",
        "# a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41XggYVRr8az",
        "colab_type": "code",
        "outputId": "80c30fd8-92d1-4d08-a1b0-cc27aab724d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "(a.var((0,2,3),True)).size()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-004f2622c63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwxNnUxmQkqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(a.var((0)).unsqueeze(-1)).size()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sphyNMNrfwQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.ones_like(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GP1UM1ufwHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaDZ92vonUN0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a.bernoulli()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YWe1Ti0MyMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=Binarize(a)\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqOWLfDtr-H2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.nn.functional.gumbel_softmax(a, tau=1, hard=True, eps=1e-10, dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu8HiOmqjwdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(a+1)/2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hkIV7ztMVyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a.tanh()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9idSm857otNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a.mean(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUGDVCa1pZes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p= np.count_nonzero((a+1)/2,axis=0)/np.count_nonzero(a,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJMpKyyqIkdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1-(p)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m-AodLmu8D3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.count_nonzero((a+1)/2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwOCJG610Shm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mu = torch.randn(5)\n",
        "sig = torch.randn(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oUI0eo0XgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=Normal(mu,sig)\n",
        "1 - x.cdf(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jw0mT8duoik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = -4.6\n",
        "v = 25936\n",
        "x = Normal(m,v)\n",
        "p = 1 - x.cdf(0)\n",
        "# s = sample_gumbel_softmax(p,1.0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjSvoApwIVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = torch.empty(3, 5)\n",
        "nn.init.uniform_(w,-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQjhRs_wIPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZEDswQu1TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aa = Normal(m,v)\n",
        "# aa.sample(torch.tensor([20]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwtW9ld5_tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5baFgYuu39G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1 - x.cdf(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAMjkieNIlS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = RelaxedOneHotCategorical(torch.tensor([1.]),a)\n",
        "m.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEYyrZzMZkuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"# # Execute this code block to install dependencies when running on colab\n",
        "# try:\n",
        "#     import torch\n",
        "# except:\n",
        "#     from os.path import exists\n",
        "#     from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "#     platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "#     cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "#     accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#     !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "# try: \n",
        "#     import torchbearer\n",
        "# except:\n",
        "#     !pip install torchbearer\n",
        "    \n",
        "# from torchbearer import Trial\n",
        "# torchbearer_trial = Trial(model, optimizer, criterion, metrics=['loss', 'accuracy']).to('cuda:0')\n",
        "# torchbearer_trial.with_generators(train_loader, test_generator=test_loader)\n",
        "# torchbearer_trial.run(epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}