{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pbnn_mnist_v8.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YM3-zGxLa63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import pdb\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import time\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli,LogitRelaxedBernoulli\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def Binarize(tensor,quant_mode='det'):\n",
        "    if quant_mode=='det':\n",
        "      tensor = tensor.sign()\n",
        "#       tensor[tensor==0] = 1\n",
        "      return tensor\n",
        "    else:\n",
        "        return tensor.add_(1).div_(2).add_(torch.rand(tensor.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "\n",
        "def sample2(mu, log_sigma2):\n",
        "    eps = torch.randn_like(mu)\n",
        "    s = mu + torch.exp(log_sigma2 / 2) * eps\n",
        "    return s\n",
        "  \n",
        "  \n",
        "def sample_gumbel(shape, eps=1e-20):\n",
        "    unif = torch.rand(*shape).cuda()\n",
        "    g = -torch.log(-torch.log(unif + eps))\n",
        "    return g\n",
        "\n",
        "def sample_gumbel_softmax(logits, temperature):\n",
        "    \"\"\"\n",
        "        Input:\n",
        "        logits: Tensor of log probs, shape = BS x k\n",
        "        temperature = scalar\n",
        "        \n",
        "        Output: Tensor of values sampled from Gumbel softmax.\n",
        "                These will tend towards a one-hot representation in the limit of temp -> 0\n",
        "                shape = BS x k\n",
        "    \"\"\"\n",
        "    g = sample_gumbel(logits.shape)\n",
        "    h = (g + logits)/temperature\n",
        "    h_max = h.max(dim=-1, keepdim=True)[0]\n",
        "    h = h - h_max\n",
        "    cache = torch.exp(h)\n",
        "    y = cache / cache.sum(dim=-1, keepdim=True)\n",
        "    return y\n",
        "  \n",
        "def sampling(mu,sig):\n",
        "  x = Normal(mu,sig)\n",
        "#   x = x.sample(torch.tensor([out_features]))\n",
        "#   print(x.cdf)\n",
        "  p = 1 - x.cdf(0)\n",
        "#   print((x.cdf(0))[0])\n",
        "#   p = Binarize(p)\n",
        "#   print(p[0])\n",
        "#   a = ((p+1)/2).bernoulli()\n",
        "#   a = a*2-1\n",
        "# #   print(a[0])\n",
        "#   a = torch.nn.functional.gumbel_softmax(p, tau=1, hard=True, eps=1e-10, dim=-1)\n",
        "#   \n",
        "#   l = LogitRelaxedBernoulli(torch.tensor([1.]).cuda(),p)\n",
        "#   l = l.sample()\n",
        "#   a = sample_gumbel_softmax(p,1.0)\n",
        "#   print(x[0]) \n",
        "  return p\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn._functions as tnnf\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "class PBinarizeLinear(nn.Linear):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(PBinarizeLinear, self).__init__(*kargs, **kwargs)\n",
        "#         w = torch.empty_like(self.weight)\n",
        "#         self.weight.data = nn.init.uniform_(w,-1,1)\n",
        "#         theta.requires_grad_\n",
        "#         self.weight.data = ((theta+1)/2).bernoulli()\n",
        "#         self.weight.data = Binarize(self.weight.data-0.5)\n",
        "#         self.weight.data = Binarize(theta)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "#         print(input.data[0])\n",
        "      \n",
        "      \n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()  \n",
        "            \n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "#         print(self.weight.data)\n",
        "#         print(self.weight.org)\n",
        "#         theta = self.weight\n",
        "        theta = torch.tanh(self.weight)\n",
        "#         print(theta)\n",
        "#         print(input[0])\n",
        "        \n",
        "\n",
        "#         print(input[0])\n",
        "        if input.size(1) != 784:\n",
        "          mu = nn.functional.linear(input,theta)\n",
        "          left = input**2 - (1- input**2)\n",
        "          right = theta**2 - (1-theta**2)\n",
        "          sigma = 1 - nn.functional.linear(left,right)\n",
        "        else:\n",
        "#           print((input**2)[0])\n",
        "#           print((1-(theta**2))[0])\n",
        "          mu = nn.functional.linear(input,theta)       \n",
        "          sigma = nn.functional.linear(input**2,1-(theta**2))\n",
        "        \n",
        "#         \n",
        "#         print(mu.shape)\n",
        "        m = mu.mean(0,True)\n",
        "        \n",
        "        v = sigma.var(0,True)\n",
        "     \n",
        "        mu = 0.5*(mu-m)/((v+(0.0001)).sqrt()+0.5)\n",
        "        sigma = 0.5**2*sigma/(v+0.0001)\n",
        "\n",
        "        \n",
        "       \n",
        "        out1 = sampling(mu,sigma)\n",
        "\n",
        "        if self.out_features==10:\n",
        "          return mu\n",
        "        else:\n",
        "          return out1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PBinarizeConv2d(nn.Conv2d):\n",
        "\n",
        "    def __init__(self, *kargs, **kwargs):\n",
        "        super(PBinarizeConv2d, self).__init__(*kargs, **kwargs)\n",
        "        \n",
        "\n",
        "    def forward(self, input):\n",
        "      \n",
        "        if not hasattr(self.weight,'org'):\n",
        "            self.weight.org=self.weight.data.clone()  \n",
        "            \n",
        "        self.weight.data=Binarize(self.weight.org)\n",
        "        \n",
        "        theta = torch.tanh(self.weight)\n",
        "        \n",
        "\n",
        "        if input.size(1) != 3:\n",
        "          mu = nn.functional.conv2d(input, theta, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "          left = input**2 - (1- input**2)\n",
        "          right = theta**2 - (1-theta**2)\n",
        "          sigma = 1 - nn.functional.conv2d(left, right, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "        else:\n",
        "          mu = nn.functional.conv2d(input, theta, None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "          sigma = nn.functional.conv2d(input**2, 1-(theta**2), None, self.stride,\n",
        "                                   self.padding, self.dilation, self.groups)\n",
        "        \n",
        "#         print(mu.shape)\n",
        "        m = mu.mean((0,2,3),True)\n",
        "        \n",
        "        v = sigma.var((0,2,3)).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "     \n",
        "        mu = 0.5*(mu-m)/((v+(0.0001)).sqrt()+0.5)\n",
        "        sigma = 0.5**2*sigma/(v+0.0001)\n",
        "        \n",
        "        \n",
        "        eps = torch.randn_like(mu)\n",
        "        s = mu + torch.exp(sigma / 2) * eps\n",
        "        \n",
        "        \n",
        "        mp, indices = nn.functional.max_pool2d(s, 2, 2,0, 1, False,True)\n",
        "                            \n",
        "        \n",
        "#         m = nn.MaxPool2d(kernel_size=2, stride=2,return_indices=True)\n",
        "#         mp, indices = m(s)\n",
        "        indices = indices.view(-1)\n",
        "        \n",
        "        mu2 = (mu.view(-1)).index_select(0,indices)  \n",
        "        mu2 = mu2.view(mp.size())\n",
        "        \n",
        "        sigma2 = (sigma.view(-1)).index_select(0,indices)  \n",
        "        sigma2 = sigma2.view(mp.size())\n",
        "\n",
        "        \n",
        "        \n",
        "        out1 = sampling(mu,sigma)\n",
        "\n",
        "\n",
        "#         if not self.bias is None:\n",
        "#             self.bias.org=self.bias.data.clone()\n",
        "#             out += self.bias.view(1, -1, 1, 1).expand_as(out)\n",
        "        \n",
        "        \n",
        "        return out1\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggHwTvf4MRqC",
        "colab_type": "code",
        "outputId": "2fac69c3-4c93-43f0-98c3-8577df3dc275",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from tqdm import tqdm\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.relaxed_bernoulli import RelaxedBernoulli\n",
        "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical\n",
        "\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "# if args.cuda:\n",
        "#     torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "# kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 32C3 - MP2 - 64C3 - Mp2 - 512FC - SM10c\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.conv1 = PBinarizeConv2d(1, 32, kernel_size=3)\n",
        "#         self.mp1= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "      \n",
        "        self.conv2 = PBinarizeConv2d(32, 64, kernel_size=3)\n",
        "#         self.mp2= nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "     \n",
        "        self.fc1 = PBinarizeLinear(36864, 512)\n",
        "\n",
        "        \n",
        "        self.fc2 = PBinarizeLinear(512, 10)\n",
        "\n",
        "\n",
        "    # 32C3 - MP2 - 64C3 - Mp2 - 512FC - SM10c\n",
        "  \n",
        "    def forward(self, x):\n",
        "      \n",
        "#       print(x.shape)\n",
        "        \n",
        "      x = self.conv1(x)\n",
        "      x = self.conv2(x)\n",
        "\n",
        "      x = x.view(x.size(0), -1)\n",
        "#         print(x.size())\n",
        "\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "\n",
        "\n",
        "      return x\n",
        "  \n",
        "\n",
        "model = Net()\n",
        "\n",
        "print(model)\n",
        "\n",
        "torch.cuda.device('cuda')\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    \n",
        "    losses = []\n",
        "    trainloader = tqdm(train_loader)\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(trainloader):\n",
        " \n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "#         print(output)\n",
        "#         output = output+1e-10\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "#         print(loss)\n",
        "\n",
        "#         if epoch%40==0:\n",
        "#             optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*0.1\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         \n",
        "        loss.backward()\n",
        "    \n",
        "        for p in list(model.parameters()):\n",
        "            if hasattr(p,'org'):\n",
        "                p.data.copy_(p.org)\n",
        "        optimizer.step()\n",
        "        \n",
        "        for p in list(model.parameters()):\n",
        "            if hasattr(p,'org'):\n",
        "                p.org.copy_(p.data.clamp_(-0.9,0.9))\n",
        "    \n",
        "        losses.append(loss.item())\n",
        "        trainloader.set_postfix(loss=np.mean(losses), epoch=epoch)\n",
        "\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    testloader = tqdm(test_loader)\n",
        "    for data, target in testloader:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "        with torch.no_grad():\n",
        "          data = Variable(data)\n",
        "        target = Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += criterion(output, target).item() # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        \n",
        "        \n",
        "\n",
        "        testloader.set_postfix(loss=test_loss / len(test_loader.dataset),acc=str((100. *correct / len(test_loader.dataset)).numpy())+'%')\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): PBinarizeConv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): PBinarizeConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): PBinarizeLinear(in_features=36864, out_features=512, bias=True)\n",
            "  (fc2): PBinarizeLinear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxgRGAoMbkCf",
        "colab_type": "code",
        "outputId": "0518981e-d181-435f-8034-bf5708df3799",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "%%%time\n",
        "for epoch in range(20):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 469/469 [00:37<00:00, 12.98it/s, epoch=0, loss=0.579]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.80it/s, acc=93%, loss=0.00173]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.69it/s, epoch=1, loss=0.182]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.83it/s, acc=95%, loss=0.00114]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.49it/s, epoch=2, loss=0.114]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.93it/s, acc=96%, loss=0.000918]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.71it/s, epoch=3, loss=0.0984]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.59it/s, acc=96%, loss=0.000846]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.52it/s, epoch=4, loss=0.0867]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.90it/s, acc=96%, loss=0.000801]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.41it/s, epoch=5, loss=0.0765]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.03it/s, acc=96%, loss=0.000804]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.51it/s, epoch=6, loss=0.0723]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.81it/s, acc=97%, loss=0.000713]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.72it/s, epoch=7, loss=0.0596]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.89it/s, acc=96%, loss=0.000846]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.52it/s, epoch=8, loss=0.0509]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.22it/s, acc=97%, loss=0.000747]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.73it/s, epoch=9, loss=0.0507]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.94it/s, acc=97%, loss=0.00058]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.51it/s, epoch=10, loss=0.0464]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.77it/s, acc=97%, loss=0.000593]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.72it/s, epoch=11, loss=0.0441]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.92it/s, acc=96%, loss=0.000898]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.45it/s, epoch=12, loss=0.0442]\n",
            "100%|██████████| 79/79 [00:03<00:00, 25.95it/s, acc=97%, loss=0.000581]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.49it/s, epoch=13, loss=0.0422]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.16it/s, acc=97%, loss=0.000686]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.53it/s, epoch=14, loss=0.0352]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.07it/s, acc=97%, loss=0.000619]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.73it/s, epoch=15, loss=0.032]\n",
            "100%|██████████| 79/79 [00:02<00:00, 28.11it/s, acc=97%, loss=0.000719]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.51it/s, epoch=16, loss=0.0391]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.96it/s, acc=97%, loss=0.000699]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.74it/s, epoch=17, loss=0.0305]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.88it/s, acc=97%, loss=0.000703]\n",
            "100%|██████████| 469/469 [00:37<00:00, 12.51it/s, epoch=18, loss=0.0275]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.88it/s, acc=97%, loss=0.000577]\n",
            "100%|██████████| 469/469 [00:36<00:00, 12.71it/s, epoch=19, loss=0.0321]\n",
            "100%|██████████| 79/79 [00:02<00:00, 27.96it/s, acc=96%, loss=0.000817]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9min 11s, sys: 3min 46s, total: 12min 58s\n",
            "Wall time: 13min 22s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiDjIjU6Mf_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.rand(4,3,12,12)\n",
        "# a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzHpcuL_q3xb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3a5683b-256b-4b2f-cb3c-7e6e42fe7d53"
      },
      "source": [
        "(a.var((0,2,3))).size()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq9fR8fOdJKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = nn.MaxPool2d(kernel_size=2, stride=2,return_indices=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFfp_kCCda_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "c = m(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsSxE6PfdvAc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "60023775-d7e5-41a5-e6e6-a2fb60b2135a"
      },
      "source": [
        "c[1].size()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 6, 6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVBrC4VBfeAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = torch.randn(4,3,12,12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhAmUt4ChN70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = b.view(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3oa3J5efjhb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = c[1].view(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GS8DR93jcMK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4854062e-38df-4fa1-d3b8-63ed746e8cab"
      },
      "source": [
        "y.size()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([432])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1xBeGTpfjei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "g = z.index_select(0,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UykM4J9OfjbX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2746
        },
        "outputId": "c0f2faa1-bffd-4971-ffce-f5e18756970c"
      },
      "source": [
        "g.view(c[1].size())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[-1.2636e+00,  9.6282e-01,  2.3601e-02, -1.5993e+00, -9.1462e-01,\n",
              "           -5.3467e-01],\n",
              "          [ 1.9239e+00,  1.2761e+00, -6.8907e-01, -4.9525e-01, -7.7302e-01,\n",
              "           -4.4493e-01],\n",
              "          [-7.0929e-01, -1.0349e-03, -3.2382e-01, -4.6858e-01, -5.7909e-01,\n",
              "            4.0405e-01],\n",
              "          [-6.6826e-01, -5.9336e-01, -1.5763e+00,  1.3286e+00, -8.7460e-02,\n",
              "           -1.6614e-01],\n",
              "          [ 6.3922e-01, -6.1810e-01, -1.5021e-01,  3.4608e-02,  1.0347e+00,\n",
              "           -8.9240e-01],\n",
              "          [-6.7547e-01,  6.2669e-01,  6.4222e-01,  1.1431e+00,  1.0325e+00,\n",
              "           -9.1363e-01]],\n",
              "\n",
              "         [[-1.5695e+00, -5.2523e-01,  2.3601e-02, -4.9735e-01, -4.3570e-01,\n",
              "           -1.5464e+00],\n",
              "          [-1.5172e+00, -6.7131e-02,  6.9641e-01, -4.9525e-01, -7.7302e-01,\n",
              "            6.1572e-01],\n",
              "          [-5.6152e-01, -1.0349e-03, -3.2382e-01,  5.3469e-01, -5.7909e-01,\n",
              "            4.0405e-01],\n",
              "          [-5.0737e-01, -6.7643e-02,  2.6314e+00,  1.3286e+00, -4.7099e-01,\n",
              "           -1.6614e-01],\n",
              "          [ 2.5327e-01,  1.1112e+00, -1.0438e+00,  3.4608e-02,  6.0006e-01,\n",
              "           -3.1509e-02],\n",
              "          [-6.7547e-01,  9.0085e-01,  3.4381e-01,  1.1431e+00,  1.0325e+00,\n",
              "           -1.6727e+00]],\n",
              "\n",
              "         [[-1.2636e+00, -8.2935e-01,  5.1920e-01, -1.5993e+00,  2.1487e-01,\n",
              "           -1.5464e+00],\n",
              "          [ 3.1481e-01,  1.2761e+00,  6.9641e-01, -1.4515e+00, -7.4690e-01,\n",
              "           -4.4493e-01],\n",
              "          [-1.9794e-01, -1.0349e-03, -6.2537e-01, -4.6858e-01, -9.1894e-01,\n",
              "            1.1038e+00],\n",
              "          [ 3.0065e-01, -6.7643e-02,  2.6314e+00, -1.8843e-01, -6.1882e-01,\n",
              "            2.3402e-02],\n",
              "          [-2.3390e+00, -6.1810e-01, -4.0650e-01, -1.2673e-01,  3.8868e-01,\n",
              "            2.1243e-01],\n",
              "          [-4.3452e-02,  9.0085e-01,  3.4381e-01, -3.8731e-01, -1.6758e-01,\n",
              "           -1.6727e+00]]],\n",
              "\n",
              "\n",
              "        [[[-1.2636e+00,  1.3376e+00,  2.3601e-02,  1.5640e+00, -9.1462e-01,\n",
              "           -3.3858e-01],\n",
              "          [ 1.8938e+00,  1.1094e-01,  1.2239e+00,  1.1829e+00, -7.7302e-01,\n",
              "            1.3088e+00],\n",
              "          [-7.0929e-01,  4.1325e-01, -3.2382e-01,  1.6142e-01, -9.1894e-01,\n",
              "            7.7140e-01],\n",
              "          [ 4.0786e-01, -4.4887e-01, -1.2743e+00,  1.3286e+00,  1.5194e+00,\n",
              "           -1.6614e-01],\n",
              "          [-2.3390e+00,  7.4078e-01, -4.0650e-01,  9.1875e-01,  3.8868e-01,\n",
              "           -8.9240e-01],\n",
              "          [-6.7547e-01,  6.2669e-01,  4.3994e-01, -9.1618e-01, -2.4351e-01,\n",
              "            1.3083e+00]],\n",
              "\n",
              "         [[-1.2636e+00,  1.3376e+00,  5.1920e-01, -1.5993e+00, -9.1462e-01,\n",
              "           -2.0159e-01],\n",
              "          [ 3.1481e-01,  1.2761e+00, -6.8907e-01,  2.0923e+00, -7.4690e-01,\n",
              "           -2.2130e+00],\n",
              "          [-1.9794e-01, -6.3800e-01, -6.2537e-01,  5.3469e-01, -9.1894e-01,\n",
              "           -1.6012e+00],\n",
              "          [ 4.0786e-01, -4.4887e-01, -1.2743e+00, -5.5778e-01, -6.1882e-01,\n",
              "           -1.6614e-01],\n",
              "          [-6.2871e-01,  7.4078e-01, -1.0438e+00,  9.1875e-01,  3.4863e-03,\n",
              "           -3.1509e-02],\n",
              "          [-4.3452e-02, -1.0207e+00,  8.5254e-01, -3.8731e-01, -2.4351e-01,\n",
              "           -1.4579e-01]],\n",
              "\n",
              "         [[ 1.3634e+00, -5.2523e-01,  2.3601e-02, -1.9306e-01, -1.0675e+00,\n",
              "           -1.5464e+00],\n",
              "          [ 3.1481e-01, -6.7131e-02,  1.2239e+00,  1.1829e+00, -7.4690e-01,\n",
              "            6.1572e-01],\n",
              "          [ 1.9936e-01,  4.1325e-01, -6.2537e-01, -3.0665e-01, -9.1894e-01,\n",
              "           -1.6012e+00],\n",
              "          [ 3.0065e-01, -5.9336e-01,  2.2244e+00, -5.5778e-01, -4.7099e-01,\n",
              "           -1.6614e-01],\n",
              "          [-6.2871e-01,  7.4078e-01,  3.5258e-01,  4.3683e-01,  3.8868e-01,\n",
              "           -8.9240e-01],\n",
              "          [-8.0962e-04,  9.0085e-01,  4.3994e-01, -3.8731e-01,  1.4646e+00,\n",
              "           -1.4579e-01]]],\n",
              "\n",
              "\n",
              "        [[[ 1.3634e+00, -8.2935e-01,  2.3601e-02, -1.5993e+00, -1.0675e+00,\n",
              "           -3.3858e-01],\n",
              "          [-1.5172e+00, -6.7131e-02,  9.2492e-02,  1.1829e+00, -7.4690e-01,\n",
              "           -2.2130e+00],\n",
              "          [ 1.9936e-01, -6.3800e-01, -3.2382e-01, -3.0665e-01,  5.5533e-01,\n",
              "            1.1038e+00],\n",
              "          [-6.6826e-01, -6.7643e-02, -1.2743e+00,  2.2266e-01, -8.7460e-02,\n",
              "           -1.9217e+00],\n",
              "          [-6.2871e-01,  7.4078e-01, -1.5021e-01, -1.2673e-01,  3.8868e-01,\n",
              "            2.1243e-01],\n",
              "          [-8.0962e-04, -1.6186e-01,  8.5254e-01,  1.4952e-01,  1.0325e+00,\n",
              "           -1.6727e+00]],\n",
              "\n",
              "         [[ 6.7784e-01, -5.2523e-01, -7.6729e-02,  1.5640e+00, -4.3570e-01,\n",
              "           -1.5464e+00],\n",
              "          [ 1.8938e+00,  1.2761e+00, -6.8907e-01, -4.9525e-01,  1.2235e-01,\n",
              "           -2.2130e+00],\n",
              "          [-1.9794e-01, -1.0349e-03, -3.2382e-01,  5.3469e-01,  1.2460e+00,\n",
              "            4.0405e-01],\n",
              "          [ 3.0065e-01, -4.4887e-01, -1.2743e+00, -5.5778e-01,  1.5194e+00,\n",
              "           -1.5161e+00],\n",
              "          [-6.2871e-01, -6.1810e-01,  3.5258e-01,  9.1875e-01,  1.0347e+00,\n",
              "           -1.7420e+00],\n",
              "          [-6.7547e-01,  6.2669e-01,  8.5254e-01, -3.8731e-01, -1.6758e-01,\n",
              "           -1.6727e+00]],\n",
              "\n",
              "         [[-1.2636e+00,  1.3376e+00, -9.7445e-01,  1.5640e+00,  2.1487e-01,\n",
              "           -5.3467e-01],\n",
              "          [-1.5172e+00,  1.2761e+00,  6.9641e-01, -1.4515e+00, -7.7302e-01,\n",
              "           -2.2130e+00],\n",
              "          [-7.0929e-01,  4.1325e-01, -3.2382e-01, -3.0665e-01,  1.2460e+00,\n",
              "            7.7140e-01],\n",
              "          [ 3.0065e-01, -4.4887e-01,  2.6314e+00,  2.2266e-01, -4.7099e-01,\n",
              "           -1.9217e+00],\n",
              "          [-6.2871e-01,  1.1112e+00, -4.0650e-01,  3.4608e-02,  1.0347e+00,\n",
              "           -1.7420e+00],\n",
              "          [-4.3452e-02,  9.0085e-01,  6.4222e-01, -9.1618e-01,  1.4646e+00,\n",
              "           -1.6727e+00]]],\n",
              "\n",
              "\n",
              "        [[[-1.2636e+00,  1.3376e+00, -7.6729e-02,  1.5640e+00, -1.0675e+00,\n",
              "           -2.0159e-01],\n",
              "          [ 1.9239e+00,  1.1094e-01, -6.8907e-01, -4.9525e-01, -7.4690e-01,\n",
              "            6.1572e-01],\n",
              "          [-1.9794e-01, -1.0349e-03, -3.2382e-01,  5.3469e-01, -5.7909e-01,\n",
              "            7.7140e-01],\n",
              "          [-5.0737e-01, -5.9336e-01,  2.2244e+00,  1.3286e+00, -6.1882e-01,\n",
              "           -1.5161e+00],\n",
              "          [ 2.5327e-01,  7.4979e-01, -4.0650e-01, -1.2673e-01,  1.0347e+00,\n",
              "           -3.1509e-02],\n",
              "          [ 1.5516e+00,  9.0085e-01,  4.3994e-01,  1.4952e-01,  1.4646e+00,\n",
              "           -1.6727e+00]],\n",
              "\n",
              "         [[-1.2636e+00,  9.6282e-01, -7.6729e-02,  1.5640e+00, -4.3570e-01,\n",
              "           -2.0159e-01],\n",
              "          [ 3.1481e-01, -6.7131e-02, -6.8907e-01,  2.0923e+00,  1.2235e-01,\n",
              "            6.1572e-01],\n",
              "          [-7.0929e-01, -1.0640e+00, -6.2537e-01,  1.6142e-01,  5.5533e-01,\n",
              "            7.7140e-01],\n",
              "          [-6.6826e-01, -4.4887e-01, -1.5763e+00,  2.2266e-01, -4.7099e-01,\n",
              "            2.3402e-02],\n",
              "          [ 6.3922e-01,  7.4078e-01,  3.5258e-01,  9.1875e-01,  1.0347e+00,\n",
              "           -1.7420e+00],\n",
              "          [ 1.5516e+00, -1.0207e+00,  4.3994e-01,  1.4952e-01,  1.4646e+00,\n",
              "           -9.1363e-01]],\n",
              "\n",
              "         [[-1.5695e+00,  1.3376e+00,  2.3601e-02,  1.5640e+00, -4.3570e-01,\n",
              "           -2.0159e-01],\n",
              "          [-1.5172e+00, -1.0007e+00,  6.9641e-01,  1.1829e+00, -1.1990e+00,\n",
              "            6.1572e-01],\n",
              "          [ 1.9936e-01,  4.1325e-01, -3.2382e-01, -3.0665e-01, -9.1894e-01,\n",
              "           -1.6012e+00],\n",
              "          [ 3.0065e-01, -5.9336e-01, -1.2743e+00, -1.8843e-01,  1.5194e+00,\n",
              "           -1.5161e+00],\n",
              "          [-2.3390e+00, -6.1810e-01, -1.0438e+00,  3.4608e-02,  6.0006e-01,\n",
              "           -3.1509e-02],\n",
              "          [ 1.5516e+00, -1.0207e+00,  3.4381e-01, -3.8731e-01,  1.4646e+00,\n",
              "           -1.6727e+00]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41XggYVRr8az",
        "colab_type": "code",
        "outputId": "0450e342-776e-4229-843d-5b1f7dae6ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(a.var((0,2,3),True)).size()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwxNnUxmQkqj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3aaca4c4-cf7a-4e12-e440-b87c8561031e"
      },
      "source": [
        "(a.var((0)).unsqueeze(-1)).size()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 12, 12, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sphyNMNrfwQz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1710
        },
        "outputId": "ddd7e8e9-3c7f-4f47-8f2c-146e2707b1af"
      },
      "source": [
        "torch.ones_like(a)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GP1UM1ufwHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaDZ92vonUN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1710
        },
        "outputId": "85b727d0-1ad3-484a-e2ec-3a2b5b70da3d"
      },
      "source": [
        "a.bernoulli()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          ...,\n",
              "          [1., 0., 1.,  ..., 1., 0., 1.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 1., 0.,  ..., 1., 1., 0.]],\n",
              "\n",
              "         [[1., 0., 1.,  ..., 0., 1., 1.],\n",
              "          [0., 0., 0.,  ..., 1., 1., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
              "          ...,\n",
              "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [0., 1., 0.,  ..., 1., 1., 1.],\n",
              "          [0., 1., 1.,  ..., 1., 0., 1.]],\n",
              "\n",
              "         [[0., 0., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 0.,  ..., 0., 1., 0.],\n",
              "          [1., 1., 0.,  ..., 0., 1., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          [1., 1., 1.,  ..., 1., 0., 1.],\n",
              "          [0., 0., 1.,  ..., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 1., 1.,  ..., 0., 0., 1.],\n",
              "          [1., 1., 0.,  ..., 0., 0., 1.],\n",
              "          ...,\n",
              "          [1., 0., 1.,  ..., 1., 0., 1.],\n",
              "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
              "          [0., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[0., 1., 0.,  ..., 1., 1., 1.],\n",
              "          [0., 1., 1.,  ..., 0., 1., 1.],\n",
              "          [1., 0., 1.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 1., 1.,  ..., 1., 0., 1.],\n",
              "          [1., 0., 1.,  ..., 1., 1., 0.],\n",
              "          [0., 1., 0.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[0., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 0., 0.,  ..., 1., 1., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 0., 1.],\n",
              "          [0., 1., 0.,  ..., 0., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [1., 0., 0.,  ..., 1., 0., 1.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 1., 0.,  ..., 0., 1., 1.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 1.],\n",
              "          [0., 1., 0.,  ..., 1., 0., 1.]],\n",
              "\n",
              "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 1.],\n",
              "          [1., 0., 0.,  ..., 0., 1., 0.],\n",
              "          ...,\n",
              "          [0., 1., 0.,  ..., 1., 1., 0.],\n",
              "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [0., 0., 1.,  ..., 0., 1., 0.]],\n",
              "\n",
              "         [[0., 1., 0.,  ..., 1., 1., 1.],\n",
              "          [0., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [0., 1., 0.,  ..., 1., 0., 0.],\n",
              "          ...,\n",
              "          [1., 1., 0.,  ..., 0., 1., 0.],\n",
              "          [1., 0., 0.,  ..., 1., 1., 1.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 1.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 1., 1., 1.],\n",
              "          [0., 0., 1.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 1.],\n",
              "          ...,\n",
              "          [1., 0., 0.,  ..., 1., 1., 1.],\n",
              "          [0., 1., 1.,  ..., 1., 0., 1.],\n",
              "          [1., 0., 1.,  ..., 1., 0., 1.]],\n",
              "\n",
              "         [[1., 1., 0.,  ..., 0., 1., 1.],\n",
              "          [0., 1., 0.,  ..., 0., 1., 1.],\n",
              "          [1., 1., 0.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 1., 0.],\n",
              "          [1., 0., 1.,  ..., 1., 1., 0.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 0., 1.],\n",
              "          [0., 0., 0.,  ..., 1., 1., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 1., 1.],\n",
              "          ...,\n",
              "          [0., 0., 1.,  ..., 0., 1., 1.],\n",
              "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [1., 0., 1.,  ..., 0., 1., 0.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YWe1Ti0MyMt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1710
        },
        "outputId": "ff209409-3a2e-4176-c5b4-873148cc2330"
      },
      "source": [
        "a=Binarize(a)\n",
        "a"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqOWLfDtr-H2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1710
        },
        "outputId": "0b5b0771-8806-4070-fa3a-9a837f172247"
      },
      "source": [
        "torch.nn.functional.gumbel_softmax(a, tau=1, hard=True, eps=1e-10, dim=-1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 1.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 1.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 1., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 1.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 1., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 1., 0., 0.],\n",
              "          [0., 0., 1.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [1., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu8HiOmqjwdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1710
        },
        "outputId": "b9fd6f09-706a-4069-b076-f63adba0b8bd"
      },
      "source": [
        "(a+1)/2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
              "\n",
              "\n",
              "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
              "\n",
              "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          ...,\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
              "          [1., 1., 1.,  ..., 1., 1., 1.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hkIV7ztMVyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1710
        },
        "outputId": "6a02693e-f292-49fc-c507-5cd766d18329"
      },
      "source": [
        "a.tanh()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]]],\n",
              "\n",
              "\n",
              "        [[[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]]],\n",
              "\n",
              "\n",
              "        [[[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]]],\n",
              "\n",
              "\n",
              "        [[[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]],\n",
              "\n",
              "         [[0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          ...,\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616],\n",
              "          [0.7616, 0.7616, 0.7616,  ..., 0.7616, 0.7616, 0.7616]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9idSm857otNw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "316eef43-dca9-4517-c27a-401e8deee773"
      },
      "source": [
        "a.mean(0)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUGDVCa1pZes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p= np.count_nonzero((a+1)/2,axis=0)/np.count_nonzero(a,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJMpKyyqIkdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674
        },
        "outputId": "187674a6-b566-4f25-a3dc-80e38f535f05"
      },
      "source": [
        "1-(p)**2"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m-AodLmu8D3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7267e65-8a45-40dc-86da-45b04ca7d927"
      },
      "source": [
        "np.count_nonzero((a+1)/2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1728"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwOCJG610Shm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mu = torch.randn(5)\n",
        "sig = torch.randn(5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_oUI0eo0XgN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f19e80a2-0697-4de6-ade8-9eb0c58c5f94"
      },
      "source": [
        "x=Normal(mu,sig)\n",
        "1 - x.cdf(0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0025, 0.9992, 0.4764, 0.4444, 0.2710])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jw0mT8duoik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = -4.6\n",
        "v = 25936\n",
        "x = Normal(m,v)\n",
        "p = 1 - x.cdf(0)\n",
        "# s = sample_gumbel_softmax(p,1.0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amjSvoApwIVS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "bcbdc8a9-96da-4d01-dc0c-490c531b1adb"
      },
      "source": [
        "w = torch.empty(3, 5)\n",
        "nn.init.uniform_(w,-1,1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1520,  0.3393,  0.6580,  0.8372, -0.4762],\n",
              "        [-0.2452, -0.0465,  0.0222,  0.7341,  0.5384],\n",
              "        [-0.0112,  0.0451,  0.0851, -0.1766,  0.7771]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cQjhRs_wIPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "6f9d24a5-c9cc-4ad2-b5f1-937859e13dfd"
      },
      "source": [
        "p.sample()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-01924b088bf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sample'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AZEDswQu1TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aa = Normal(m,v)\n",
        "# aa.sample(torch.tensor([20]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwtW9ld5_tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5baFgYuu39G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1 - x.cdf(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAMjkieNIlS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = RelaxedOneHotCategorical(torch.tensor([1.]),a)\n",
        "m.sample()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEYyrZzMZkuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"# # Execute this code block to install dependencies when running on colab\n",
        "# try:\n",
        "#     import torch\n",
        "# except:\n",
        "#     from os.path import exists\n",
        "#     from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "#     platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "#     cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "#     accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#     !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "# try: \n",
        "#     import torchbearer\n",
        "# except:\n",
        "#     !pip install torchbearer\n",
        "    \n",
        "# from torchbearer import Trial\n",
        "# torchbearer_trial = Trial(model, optimizer, criterion, metrics=['loss', 'accuracy']).to('cuda:0')\n",
        "# torchbearer_trial.with_generators(train_loader, test_generator=test_loader)\n",
        "# torchbearer_trial.run(epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}